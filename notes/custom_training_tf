#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon May 25 20:51:24 2020

@author: ghiggi
"""
# Import TensorFlow
import tensorflow as tf

def create_model(input_shape):
    """Creates a simple convolutional neural network model using the Keras API"""
    model = tf.keras.Sequential([ tf.keras.layers.Conv2D(28, kernel_size=(3, 3), activation='relu', input_shape=input_shape),
                                  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                                  tf.keras.layers.Flatten(),
                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                  tf.keras.layers.Dropout(0.2),
                                  tf.keras.layers.Dense(10, activation=tf.nn.softmax),
  ])
    return 
#%% Define custom loss
def loss(model, x, y):
  """Calculates the loss given an example (x, y)"""
  logits = model(x)
  return logits, tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)
#%% Define custom gradient 
def grad(model, x, y):
  """Calculates the loss and the gradients given an example (x, y)"""
  logits, loss_value = loss(model, x, y)
  return logits, loss_value, tf.gradients(loss_value, model.trainable_variables)

#%% Define in strategy scope 
input_shape = (28,28,1)
model = create_model(input_shape)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)
training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
      'training_accuracy', dtype=tf.float32)
test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
    'test_accuracy', dtype=tf.float32)
   
def train_step(inputs):
  """Each training step runs this custom function which calculates
  gradients and updates weights.
  """
  x, y = inputs

  logits, loss_value, grads = grad(model, x, y)

  update_loss = training_loss.update_state(loss_value)
  update_accuracy = training_accuracy.update_state(y, logits)

  # Show that this is truly a custom training loop
  # Multiply all gradients by 2.
  grads = grads * 2

  update_vars = optimizer.apply_gradients(
      zip(grads, model.trainable_variables))

  with tf.control_dependencies([update_vars, update_loss, update_accuracy]):
    return tf.identity(loss_value)