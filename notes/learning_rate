#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue May 26 10:40:23 2020

@author: ghiggi
"""
import tensorflow as tf 
import tensorflow.keras as keras
from tensorflow.keras.callbacks import LearningRateScheduler
import matplotlib.pyplot as plt
#%% Add learning rates to callbacks 
lr_callback = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)
lr_callback = LearningRateScheduler(lambda x: 1e-3 * 0.8**x)
lr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)

# Pass callback to model.fit() 
# history = model.fit(training_dataset,
#                     validation_data=validation_dataset,
#                     callbacks=[lr_callback])

#%% Add LR scheduling directly to SGD 
lr_schedule = keras.optimizers.schedules.ExponentialDecay(0.05, 
                                                          decay_steps=100000, 
                                                          decay_rate=0.96)
optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)


#%% Plot LR scheduling 
def plot_LR_scheduling(lrfn, n_epochs):
    # Display Learning Rate Scheduling 
    # - Provide a LR function (lrfn)
    rng = [i for i in range(n_epochs)]
    y = [lrfn(x) for x in rng]
    plt.plot(rng, [lrfn(x) for x in rng])
    plt.xlabel('Epochs')
    plt.ylabel('Learning rate')
    plt.title('Learning rate per epoch')
    print(y[0], y[-1])

#%% Learning rate scheduling for transfer learning / fine tuning a pretrained model
start_lr = 0.00001
min_lr = 0.00001
max_lr = 0.00005  
rampup_epochs = 5
sustain_epochs = 0
exp_decay = .8

def lrfn(epoch):
    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):
        if epoch < rampup_epochs:
            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr
        elif epoch < rampup_epochs + sustain_epochs:
            lr = max_lr
        else:
            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr
        return lr
    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)
    
def lrfn(epoch):
  if epoch < rampup_epochs:
    return (max_lr - start_lr)/rampup_epochs * epoch + start_lr
  elif epoch < rampup_epochs + sustain_epochs:
    return max_lr
  else:
    return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr
    
lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)
lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)
